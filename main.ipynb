{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "554194bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openvino\\runtime\\__init__.py:10: DeprecationWarning: The `openvino.runtime` module is deprecated and will be removed in the 2026.0 release. Please replace `openvino.runtime` with `openvino`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#os.environ[\"OMP_NUM_THREADS\"] = str(os.cpu_count() // 2)\n",
    "#os.environ[\"GOMP_CPU_AFFINITY\"] = \"granularity=core,compact\"\n",
    "from Scripts.model2 import *\n",
    "from Scripts.loss import *\n",
    "from Scripts.results_manager import *\n",
    "from Scripts.plots import *\n",
    "from Scripts.dataset import *\n",
    "from Scripts.trainer import *\n",
    "from Scripts.inference import *\n",
    "from Scripts.Onnx_Class import *\n",
    "from Scripts.lr_finder import *\n",
    "from Scripts.generate_configs import *\n",
    "from Scripts.excecute import *\n",
    "from Scripts.upload_summaries import *\n",
    "from Scripts.quantize import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581e96cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"Configs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6181dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_selected_class(config_path, 'grid')\n",
    "# metrics_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e587851",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_all_classes(config_path)\n",
    "metrics_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d3e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_run_folder = \"Training_Runs\"\n",
    "inference_output_dir = \"Inference_Runs\"\n",
    "\n",
    "inference_model(training_run_folder, inference_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c1c54cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade FP32-Modelle für die Quantisierung...\n",
      "Bereite das Studenten-Modell für die Quantisierung vor (FX Graph Transformation)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erstelle den Kalibrierungs-Daten-Loader...\n",
      "Starte die Kalibrierung des Modells...\n",
      "Kalibrierung abgeschlossen.\n",
      "Konvertiere das vorbereitete Modell zum endgültigen quantisierten Modell...\n",
      "Quantisiertes Modell und Artefakte erfolgreich gespeichert unter: quantized_models\\MVTecAD_grid\\mobilenet_v3_large\\40cc0207-55f9-45d3-b20b-1cc5c5d80040\n",
      "Lade FP32-Modelle für die Quantisierung...\n",
      "Bereite das Studenten-Modell für die Quantisierung vor (FX Graph Transformation)...\n",
      "Erstelle den Kalibrierungs-Daten-Loader...\n",
      "Starte die Kalibrierung des Modells...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kalibrierung abgeschlossen.\n",
      "Konvertiere das vorbereitete Modell zum endgültigen quantisierten Modell...\n",
      "Quantisiertes Modell und Artefakte erfolgreich gespeichert unter: quantized_models\\MVTecAD_grid\\resnet18\\7804c936-9a41-467a-8dfc-e31f5c13998b\n",
      "Lade FP32-Modelle für die Quantisierung...\n",
      "Bereite das Studenten-Modell für die Quantisierung vor (FX Graph Transformation)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erstelle den Kalibrierungs-Daten-Loader...\n",
      "Starte die Kalibrierung des Modells...\n",
      "Kalibrierung abgeschlossen.\n",
      "Konvertiere das vorbereitete Modell zum endgültigen quantisierten Modell...\n",
      "Quantisiertes Modell und Artefakte erfolgreich gespeichert unter: quantized_models\\MVTecAD_grid\\shufflenet_v2_x1_0\\8f25c698-9597-4255-999c-51f15eea019f\n"
     ]
    }
   ],
   "source": [
    "config_paths_all = glob.glob(os.path.join(\n",
    "    \"Training_Runs\", \"**\", \"*.yaml\"), recursive=True)\n",
    "summary_metrics_paths_all = glob.glob(os.path.join(\n",
    "    \"Training_Runs\", \"**\", \"summary_metrics.json\"), recursive=True)\n",
    "best_student_weight_paths_all = glob.glob(os.path.join(\n",
    "    \"Training_Runs\", \"**\", \"*best_student.pth\"), recursive=True)\n",
    "\n",
    "for configs, summary_metrics, best_student_weights in zip(config_paths_all, summary_metrics_paths_all, best_student_weight_paths_all):\n",
    "    config = load_config(configs)\n",
    "    summary_metric = load_json(summary_metrics)\n",
    "    quantize_model(\n",
    "        best_student_weights,\n",
    "        config,\n",
    "        summary_metric\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ef59b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Führe Inferenz für 3 quantisierte Modelle aus...\n",
      "Modell aus mobilenet_v3_large wird verwendet...\n",
      "Lade Modelle fuer die Quantisierung...\n",
      "Bereite das Studenten-Modell fuer die Quantisierung vor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alant\\AppData\\Local\\Temp\\ipykernel_14956\\2731857662.py:50: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  prepared_model = quantize_fx.prepare_fx(\n",
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konvertiere das vorbereitete Modell in ein quantisiertes Modell...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alant\\AppData\\Local\\Temp\\ipykernel_14956\\2731857662.py:54: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  quantized_student_model = quantize_fx.convert_fx(prepared_model)\n",
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:1343: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n",
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_utils.py:465: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade die quantisierten Gewichte in das Modell...\n",
      "Lade den gesamten Test-Datensatz in den Arbeitsspeicher...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 65.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Inferenz für Konfiguration: quantized_models\\MVTecAD_grid\\mobilenet_v3_large\\40cc0207-55f9-45d3-b20b-1cc5c5d80040\\STFPM_Config_mobilenet_v3_large.yaml...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PyTorch Profiler-Analyse ---\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                  anomaly_map_inference         6.42%      44.691ms        98.08%     682.612ms     341.306ms             2  \n",
      "                                           aten::conv2d         0.09%     611.600us        27.77%     193.255ms       1.559ms           124  \n",
      "                                      aten::convolution         0.10%     717.800us        27.68%     192.643ms       1.554ms           124  \n",
      "                                     aten::_convolution         0.16%       1.110ms        27.58%     191.925ms       1.548ms           124  \n",
      "                               aten::mkldnn_convolution        27.10%     188.602ms        27.42%     190.815ms       1.539ms           124  \n",
      "                                       aten::batch_norm         0.05%     357.500us        16.84%     117.186ms       1.274ms            92  \n",
      "                           aten::_batch_norm_impl_index         0.11%     789.200us        16.79%     116.828ms       1.270ms            92  \n",
      "                                aten::native_batch_norm        16.23%     112.987ms        16.65%     115.860ms       1.259ms            92  \n",
      "                                 quantized::conv2d_relu        11.11%      77.295ms        11.15%      77.593ms       2.425ms            32  \n",
      "                                        aten::clamp_min         7.85%      54.616ms         7.85%      54.616ms       1.092ms            50  \n",
      "                                             aten::relu         0.02%     173.000us         7.83%      54.503ms       1.434ms            38  \n",
      "                                      quantized::conv2d         6.88%      47.891ms         7.10%      49.434ms     852.305us            58  \n",
      "                              aten::adaptive_avg_pool2d         0.03%     190.300us         3.40%      23.666ms     845.196us            28  \n",
      "                              aten::upsample_bilinear2d         2.95%      20.530ms         2.96%      20.579ms       3.430ms             6  \n",
      "                                         quantized::mul         2.82%      19.601ms         2.82%      19.659ms       1.966ms            10  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 695.978ms\n",
      "\n",
      "Inference summary saved to quantized_inference_results\\MVTecAD_grid\\mobilenet_v3_large\\40cc0207-55f9-45d3-b20b-1cc5c5d80040\\inference_summary.json\n",
      "Inferenz abgeschlossen. AUROC: 0.9081, Zeit: 0.6827s.\n",
      "Modell aus resnet18 wird verwendet...\n",
      "Lade Modelle fuer die Quantisierung...\n",
      "Bereite das Studenten-Modell fuer die Quantisierung vor...\n",
      "Konvertiere das vorbereitete Modell in ein quantisiertes Modell...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alant\\AppData\\Local\\Temp\\ipykernel_14956\\2731857662.py:50: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  prepared_model = quantize_fx.prepare_fx(\n",
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "C:\\Users\\alant\\AppData\\Local\\Temp\\ipykernel_14956\\2731857662.py:54: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  quantized_student_model = quantize_fx.convert_fx(prepared_model)\n",
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:1343: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade die quantisierten Gewichte in das Modell...\n",
      "Lade den gesamten Test-Datensatz in den Arbeitsspeicher...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 64.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Inferenz für Konfiguration: quantized_models\\MVTecAD_grid\\resnet18\\7804c936-9a41-467a-8dfc-e31f5c13998b\\STFPM_Config_resnet18.yaml...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alant\\AppData\\Local\\Temp\\ipykernel_14956\\2731857662.py:50: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  prepared_model = quantize_fx.prepare_fx(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PyTorch Profiler-Analyse ---\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                  anomaly_map_inference         5.37%      78.123ms        99.09%        1.440s     720.168ms             2  \n",
      "                                           aten::conv2d         0.02%     223.800us        48.10%     699.201ms      17.480ms            40  \n",
      "                                      aten::convolution         0.02%     307.200us        48.09%     698.977ms      17.474ms            40  \n",
      "                                     aten::_convolution         0.03%     398.400us        48.06%     698.670ms      17.467ms            40  \n",
      "                               aten::mkldnn_convolution        48.01%     697.931ms        48.04%     698.272ms      17.457ms            40  \n",
      "                                      quantized::conv2d        15.02%     218.268ms        15.07%     218.995ms       9.954ms            22  \n",
      "                                 quantized::conv2d_relu        11.86%     172.383ms        11.88%     172.632ms      10.790ms            16  \n",
      "                                       aten::batch_norm         0.01%     178.900us         4.10%      59.590ms       1.490ms            40  \n",
      "                           aten::_batch_norm_impl_index         0.03%     408.700us         4.09%      59.411ms       1.485ms            40  \n",
      "                                aten::native_batch_norm         4.00%      58.134ms         4.05%      58.903ms       1.473ms            40  \n",
      "                                             aten::relu         0.03%     391.500us         3.83%      55.745ms       1.640ms            34  \n",
      "                                        aten::clamp_min         3.83%      55.678ms         3.83%      55.678ms       1.114ms            50  \n",
      "                              aten::upsample_bilinear2d         1.97%      28.682ms         1.98%      28.760ms       3.595ms             8  \n",
      "                                       aten::max_pool2d         0.00%      29.400us         1.54%      22.365ms      11.182ms             2  \n",
      "                          aten::max_pool2d_with_indices         1.54%      22.335ms         1.54%      22.335ms      11.168ms             2  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.454s\n",
      "\n",
      "Inference summary saved to quantized_inference_results\\MVTecAD_grid\\resnet18\\7804c936-9a41-467a-8dfc-e31f5c13998b\\inference_summary.json\n",
      "Inferenz abgeschlossen. AUROC: 0.9916, Zeit: 1.4404s.\n",
      "Modell aus shufflenet_v2_x1_0 wird verwendet...\n",
      "Lade Modelle fuer die Quantisierung...\n",
      "Bereite das Studenten-Modell fuer die Quantisierung vor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konvertiere das vorbereitete Modell in ein quantisiertes Modell...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alant\\AppData\\Local\\Temp\\ipykernel_14956\\2731857662.py:54: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  quantized_student_model = quantize_fx.convert_fx(prepared_model)\n",
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:1343: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade die quantisierten Gewichte in das Modell...\n",
      "Lade den gesamten Test-Datensatz in den Arbeitsspeicher...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 64.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Inferenz für Konfiguration: quantized_models\\MVTecAD_grid\\shufflenet_v2_x1_0\\8f25c698-9597-4255-999c-51f15eea019f\\STFPM_Config_shufflenet_v2_x1_0.yaml...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PyTorch Profiler-Analyse ---\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                  anomaly_map_inference         4.73%      60.921ms        99.01%        1.274s     637.107ms             2  \n",
      "                                      quantized::conv2d        63.86%     821.812ms        64.21%     826.306ms      21.745ms            38  \n",
      "                                           aten::conv2d         0.04%     461.900us         9.64%     124.075ms       1.108ms           112  \n",
      "                                      aten::convolution         0.07%     836.800us         9.61%     123.613ms       1.104ms           112  \n",
      "                                     aten::_convolution         0.11%       1.438ms         9.54%     122.776ms       1.096ms           112  \n",
      "                               aten::mkldnn_convolution         7.39%      95.081ms         7.47%      96.076ms     857.821us           112  \n",
      "                                       aten::contiguous         0.03%     425.500us         4.80%      61.828ms     332.406us           186  \n",
      "                                            aten::clone         1.89%      24.381ms         4.77%      61.402ms     330.119us           186  \n",
      "                                 quantized::conv2d_relu         3.32%      42.730ms         4.54%      58.374ms     833.913us            70  \n",
      "                                              aten::cat         2.43%      31.304ms         4.35%      56.010ms     608.804us            92  \n",
      "                                            aten::copy_         3.24%      41.706ms         3.24%      41.706ms     315.953us           132  \n",
      "                                       aten::batch_norm         0.05%     595.700us         3.10%      39.885ms     356.112us           112  \n",
      "                           aten::_batch_norm_impl_index         0.07%     880.000us         3.05%      39.289ms     350.794us           112  \n",
      "                                aten::native_batch_norm         2.80%      35.982ms         2.97%      38.227ms     341.317us           112  \n",
      "                               aten::linalg_vector_norm         1.87%      24.065ms         1.87%      24.065ms       2.005ms            12  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.287s\n",
      "\n",
      "Inference summary saved to quantized_inference_results\\MVTecAD_grid\\shufflenet_v2_x1_0\\8f25c698-9597-4255-999c-51f15eea019f\\inference_summary.json\n",
      "Inferenz abgeschlossen. AUROC: 0.6249, Zeit: 1.2743s.\n",
      "\n",
      "--- Alle Inferenzläufe abgeschlossen. ---\n"
     ]
    }
   ],
   "source": [
    "inference_output_dir = 'quantized_inference_results'\n",
    "device = torch.device('cpu')\n",
    "\n",
    "quantized_weights_paths = glob.glob(os.path.join(\n",
    "    'quantized_models', '**', 'quantized_model.pth'), recursive=True)\n",
    "\n",
    "print(\n",
    "    f\"Führe Inferenz für {len(quantized_weights_paths)} quantisierte Modelle aus...\")\n",
    "\n",
    "for weight_path in quantized_weights_paths:\n",
    "    dir_path = os.path.dirname(weight_path)\n",
    "    model_name = Path(weight_path).parent.parent.name\n",
    "    print(f\"Modell aus {model_name} wird verwendet...\")\n",
    "\n",
    "    yaml_filename = None\n",
    "    for file in os.listdir(dir_path):\n",
    "        if file.endswith('.yaml'):\n",
    "            yaml_filename = file\n",
    "            break\n",
    "\n",
    "    if yaml_filename is None:\n",
    "        print(\n",
    "            f\"Keine YAML-Konfigurationsdatei im Verzeichnis {dir_path} gefunden.\")\n",
    "        continue\n",
    "\n",
    "    json_path = os.path.join(dir_path, 'summary_metric.json')\n",
    "    yaml_path = os.path.join(dir_path, yaml_filename)\n",
    "\n",
    "    if not os.path.exists(json_path):\n",
    "        print(\n",
    "            f\"Keine JSON-Zusammenfassungsdatei im Verzeichnis {dir_path} gefunden.\")\n",
    "        continue\n",
    "\n",
    "    config = load_config(yaml_path)\n",
    "    summary_data = load_json(json_path)\n",
    "    training_id = summary_data.get('training_id', 'quantized_run')\n",
    "    print(\"Lade Modelle fuer die Quantisierung...\")\n",
    "    inference_model = STFPM(\n",
    "        architecture=config['model']['architecture'],\n",
    "        layers=config['model']['layers'],\n",
    "        quantize=False\n",
    "    ).to(device).eval()\n",
    "    stem_model = inference_model.stem_model\n",
    "    student_model_to_quantize = inference_model.student_model.to(device).eval()\n",
    "    qconfig_mapping = get_default_qconfig_mapping('fbgemm')\n",
    "    example_inputs = (stem_model(torch.randn(\n",
    "        1, 3, config['dataset']['img_size'], config['dataset']['img_size']\n",
    "    )),)\n",
    "    print(\"Bereite das Studenten-Modell fuer die Quantisierung vor...\")\n",
    "    prepared_model = quantize_fx.prepare_fx(\n",
    "        student_model_to_quantize, qconfig_mapping, example_inputs\n",
    "    )\n",
    "    print(\"Konvertiere das vorbereitete Modell in ein quantisiertes Modell...\")\n",
    "    quantized_student_model = quantize_fx.convert_fx(prepared_model)\n",
    "    print(\"Lade die quantisierten Gewichte in das Modell...\")\n",
    "    quantized_student_model.load_state_dict(\n",
    "        torch.load(weight_path, map_location=device)\n",
    "    )\n",
    "    quantized_student_model.eval()\n",
    "\n",
    "    inference_model.student_model = quantized_student_model.to(device)\n",
    "\n",
    "    try:\n",
    "        test_set = MVTecDataset(\n",
    "            img_size=config['dataset']['img_size'],\n",
    "            base_path=config['dataset']['base_path'],\n",
    "            cls=config['dataset']['class'],\n",
    "            mode='test',\n",
    "            download_if_missing=False\n",
    "        )\n",
    "        print(\"Lade den gesamten Test-Datensatz in den Arbeitsspeicher...\")\n",
    "        memory_cache = [test_set[i] for i in tqdm(range(len(test_set)))]\n",
    "        test_loader = DataLoader(\n",
    "            memory_cache,\n",
    "            batch_size=config['dataloader']['batch_size'],\n",
    "            shuffle=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden des Test-Datensatzes für {yaml_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    infer = Inference(\n",
    "        model=inference_model,\n",
    "        test_loader=test_loader,\n",
    "        config=config,\n",
    "        output_dir=inference_output_dir,\n",
    "        path_to_student_weight=None,\n",
    "        trainings_id=training_id,\n",
    "        inferenz=True\n",
    "    )\n",
    "\n",
    "    print(f\"Starte Inferenz für Konfiguration: {yaml_path}...\")\n",
    "    auroc_score, total_inference_time = infer.evaluate_quantized_loaded_model()\n",
    "    infer.create_inference_summary(\n",
    "        summary_data, auroc_score, total_inference_time)\n",
    "    print(\n",
    "        f\"Inferenz abgeschlossen. AUROC: {auroc_score:.4f}, Zeit: {total_inference_time:.4f}s.\")\n",
    "\n",
    "    # infer.generate_heatmaps_from_saved_maps()\n",
    "\n",
    "print(\"\\n--- Alle Inferenzläufe abgeschlossen. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "798d03d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Modelle fuer die Quantisierung...\n",
      "Bereite das Studenten-Modell fuer die Quantisierung vor...\n",
      "Konvertiere das vorbereitete Modell in ein quantisiertes Modell...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alant\\AppData\\Local\\Temp\\ipykernel_4656\\1780065461.py:16: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  prepared_model = quantize_fx.prepare_fx(\n",
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "C:\\Users\\alant\\AppData\\Local\\Temp\\ipykernel_4656\\1780065461.py:20: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  quantized_student_model = quantize_fx.convert_fx(prepared_model)\n",
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:1343: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade die quantisierten Gewichte in das Modell...\n"
     ]
    }
   ],
   "source": [
    "config = load_config(\n",
    "    r'quantized_models\\MVTecAD_grid\\resnet18\\7804c936-9a41-467a-8dfc-e31f5c13998b\\STFPM_Config_resnet18.yaml')\n",
    "print(\"Lade Modelle fuer die Quantisierung...\")\n",
    "inference_model = STFPM(\n",
    "    architecture=config['model']['architecture'],\n",
    "    layers=config['model']['layers'],\n",
    "    quantize=False\n",
    ").to(device).eval()\n",
    "stem_model = inference_model.stem_model\n",
    "student_model_to_quantize = inference_model.student_model.to(device).eval()\n",
    "qconfig_mapping = get_default_qconfig_mapping('fbgemm')\n",
    "example_inputs = (stem_model(torch.randn(\n",
    "    1, 3, config['dataset']['img_size'], config['dataset']['img_size']\n",
    ")),)\n",
    "print(\"Bereite das Studenten-Modell fuer die Quantisierung vor...\")\n",
    "prepared_model = quantize_fx.prepare_fx(\n",
    "    student_model_to_quantize, qconfig_mapping, example_inputs\n",
    ")\n",
    "print(\"Konvertiere das vorbereitete Modell in ein quantisiertes Modell...\")\n",
    "quantized_student_model = quantize_fx.convert_fx(prepared_model)\n",
    "print(\"Lade die quantisierten Gewichte in das Modell...\")\n",
    "quantized_student_model.load_state_dict(\n",
    "    torch.load(r'quantized_models\\MVTecAD_grid\\resnet18\\7804c936-9a41-467a-8dfc-e31f5c13998b\\quantized_model.pth', map_location=device)\n",
    ")\n",
    "quantized_student_model.eval()\n",
    "\n",
    "inference_model.student_model = quantized_student_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad1f74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(r'Configs\\resnet18\\grid\\STFPM_Config_resnet18_grid.yaml')\n",
    "model = STFPM(\n",
    "    architecture=config['model']['architecture'],\n",
    "    layers=config['model']['layers'],\n",
    "    quantize=True\n",
    ")\n",
    "print(model.teacher_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b66448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import time  # Hinzugefügt: Modul für die Zeitmessung\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. ONNX-Modell laden\n",
    "onnx_model_path = r\"onnx_models\\STFPM_bottle_mobilenetv4_conv_large.onnx\"\n",
    "sess = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "image_path = glob.glob(os.path.join(\n",
    "    r'Images\\bottle\\test\\broken_large', '*.png'))\n",
    "for path in image_path:\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "    img_size = 256\n",
    "    image = image.resize((img_size, img_size))\n",
    "\n",
    "    input_data = np.array(image, dtype=np.uint8)\n",
    "    input_data = np.expand_dims(input_data, axis=0)\n",
    "\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    outputs = sess.run(None, {input_name: input_data})\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    inference_time = end_time - start_time\n",
    "\n",
    "    anomaly_map = outputs[0]\n",
    "    anomaly_score = outputs[1]\n",
    "\n",
    "    print(f\"Anomalie-Score für das Bild: {anomaly_score[0]}\")\n",
    "    print(f\"Inferenzzeit: {inference_time:.4f} Sekunden\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # 2. Originalbild anzeigen\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Originalbild\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # 3. Heatmap über das Originalbild legen\n",
    "    axes[1].imshow(image)\n",
    "    heatmap = axes[1].imshow(np.squeeze(anomaly_map), cmap='jet', alpha=0.5)\n",
    "    axes[1].set_title(\"Anomalie-Heatmap\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # 4. Farbbalken für die Heatmap hinzufügen\n",
    "    fig.colorbar(heatmap, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    # 5. Plot anzeigen oder speichern\n",
    "    plt.suptitle(\"Vergleich: Originalbild vs. Anomalie-Heatmap\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
