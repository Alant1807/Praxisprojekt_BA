{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "554194bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openvino\\runtime\\__init__.py:10: DeprecationWarning: The `openvino.runtime` module is deprecated and will be removed in the 2026.0 release. Please replace `openvino.runtime` with `openvino`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#os.environ[\"OMP_NUM_THREADS\"] = str(os.cpu_count() // 2)\n",
    "#os.environ[\"GOMP_CPU_AFFINITY\"] = \"granularity=core,compact\"\n",
    "from Scripts.model2 import *\n",
    "from Scripts.loss import *\n",
    "from Scripts.results_manager import *\n",
    "from Scripts.plots import *\n",
    "from Scripts.dataset import *\n",
    "from Scripts.trainer import *\n",
    "from Scripts.inference import *\n",
    "from Scripts.Onnx_Class import *\n",
    "from Scripts.lr_finder import *\n",
    "from Scripts.generate_configs import *\n",
    "from Scripts.excecute import *\n",
    "from Scripts.upload_summaries import *\n",
    "#from Scripts.quantize import *\n",
    "from Scripts.quantize2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581e96cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"Configs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6181dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_selected_class(config_path, 'grid')\n",
    "# metrics_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e587851",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_all_classes(config_path)\n",
    "metrics_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d3e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_run_folder = \"Training_Runs\"\n",
    "inference_output_dir = \"Inference_Runs\"\n",
    "\n",
    "inference_model(training_run_folder, inference_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c1c54cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Modelle fuer die Quantisierung...\n",
      "Bereite das Studenten-Modell fuer die Quantisierung vor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erstelle den Kalibrierungs-Daten-Loader...\n",
      "Kalibriere das Modell...\n",
      "Kalibrierung abgeschlossen.\n",
      "Konvertiere das zum quantisierten Modell...\n",
      "Quantisiertes Modell und Artefakte gespeichert unter: quantized_models\\MVTecAD_grid\\mobilenet_v3_large\\40cc0207-55f9-45d3-b20b-1cc5c5d80040\n",
      "Lade Modelle fuer die Quantisierung...\n",
      "Bereite das Studenten-Modell fuer die Quantisierung vor...\n",
      "Erstelle den Kalibrierungs-Daten-Loader...\n",
      "Kalibriere das Modell...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kalibrierung abgeschlossen.\n",
      "Konvertiere das zum quantisierten Modell...\n",
      "Quantisiertes Modell und Artefakte gespeichert unter: quantized_models\\MVTecAD_grid\\resnet18\\7804c936-9a41-467a-8dfc-e31f5c13998b\n",
      "Lade Modelle fuer die Quantisierung...\n",
      "Bereite das Studenten-Modell fuer die Quantisierung vor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erstelle den Kalibrierungs-Daten-Loader...\n",
      "Kalibriere das Modell...\n",
      "Kalibrierung abgeschlossen.\n",
      "Konvertiere das zum quantisierten Modell...\n",
      "Quantisiertes Modell und Artefakte gespeichert unter: quantized_models\\MVTecAD_grid\\shufflenet_v2_x1_0\\8f25c698-9597-4255-999c-51f15eea019f\n"
     ]
    }
   ],
   "source": [
    "config_paths_all = glob.glob(os.path.join(\n",
    "    \"Training_Runs\", \"**\", \"*.yaml\"), recursive=True)\n",
    "summary_metrics_paths_all = glob.glob(os.path.join(\n",
    "    \"Training_Runs\", \"**\", \"summary_metrics.json\"), recursive=True)\n",
    "best_student_weight_paths_all = glob.glob(os.path.join(\n",
    "    \"Training_Runs\", \"**\", \"*best_student.pth\"), recursive=True)\n",
    "\n",
    "for configs, summary_metrics, best_student_weights in zip(config_paths_all, summary_metrics_paths_all, best_student_weight_paths_all):\n",
    "    config = load_config(configs)\n",
    "    summary_metric = load_json(summary_metrics)\n",
    "    quantize_model(\n",
    "        best_student_weights,\n",
    "        config,\n",
    "        summary_metric\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef59b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Führe Inferenz für 3 quantisierte Modelle aus...\n",
      "Modell aus mobilenet_v3_large wird verwendet...\n",
      "Lade Modelle fuer die Quantisierung...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\ao\\quantization\\utils.py:409: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n",
      "C:\\Users\\alant\\AppData\\Local\\Temp\\ipykernel_3352\\1185449681.py:50: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  prepared_model = quantize_fx.prepare_fx(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bereite das Studenten-Modell fuer die Quantisierung vor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konvertiere das vorbereitete Modell in ein quantisiertes Modell...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alant\\AppData\\Local\\Temp\\ipykernel_3352\\1185449681.py:54: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  quantized_student_model = quantize_fx.convert_fx(prepared_model)\n",
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:1343: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n",
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_utils.py:465: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade die quantisierten Gewichte in das Modell...\n",
      "Starte Inferenz für Konfiguration: quantized_models\\MVTecAD_grid\\mobilenet_v3_large\\40cc0207-55f9-45d3-b20b-1cc5c5d80040\\STFPM_Config_mobilenet_v3_large.yaml...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mStopIteration\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 91\u001b[39m\n\u001b[32m     80\u001b[39m infer = Inference(\n\u001b[32m     81\u001b[39m     model=inference_model,\n\u001b[32m     82\u001b[39m     test_loader=test_loader,\n\u001b[32m   (...)\u001b[39m\u001b[32m     87\u001b[39m     inferenz=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     88\u001b[39m )\n\u001b[32m     90\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarte Inferenz für Konfiguration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myaml_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m auroc_score, total_inference_time = \u001b[43minfer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate_loaded_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m infer.create_inference_summary(\n\u001b[32m     93\u001b[39m     summary_data, auroc_score, total_inference_time)\n\u001b[32m     94\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     95\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInferenz abgeschlossen. AUROC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauroc_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Zeit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_inference_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\Praxisprojekt_BA\\Scripts\\inference.py:127\u001b[39m, in \u001b[36mInference.evaluate_loaded_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    125\u001b[39m start_time = time.perf_counter()\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# with torch.autocast(device_type=self.device.type):\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m anomaly_map = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43manomaly_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    130\u001b[39m     torch.cuda.synchronize()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\Praxisprojekt_BA\\Scripts\\model2.py:82\u001b[39m, in \u001b[36mSTFPM.anomaly_map\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\" \u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[33;03mBerechnet die Anomalie-Karte für einen Batch von Bildern.\u001b[39;00m\n\u001b[32m     73\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m \u001b[33;03m    torch.Tensor: Eine Anomalie-Karte der Form (B, H, W), die die Anomalien im Bild darstellt.\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# x ist ein Batch von Bildern, z.B. (B, C, H, W)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m teacher_feature_maps, student_feature_maps = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m batch_size, _, img_height, img_width = x.shape\n\u001b[32m     85\u001b[39m anomaly_map = torch.ones(\n\u001b[32m     86\u001b[39m     (batch_size, img_height, img_width), device=x.device\n\u001b[32m     87\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\Praxisprojekt_BA\\Scripts\\model2.py:141\u001b[39m, in \u001b[36mSTFPM.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    138\u001b[39m stem_output = \u001b[38;5;28mself\u001b[39m.stem_model(x)\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     teacher_feature_maps = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstem_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m student_feature_maps = \u001b[38;5;28mself\u001b[39m.student_model(stem_output)\n\u001b[32m    144\u001b[39m teacher_feature_maps = \u001b[38;5;28mlist\u001b[39m(teacher_feature_maps.values())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\Praxisprojekt_BA\\Scripts\\model2.py:189\u001b[39m, in \u001b[36mFeatureExtractor.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    187\u001b[39m     \u001b[38;5;28mself\u001b[39m.features.clear()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.requires_grad:\n\u001b[32m    190\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    191\u001b[39m             _ = \u001b[38;5;28mself\u001b[39m.model(x)\n",
      "\u001b[31mStopIteration\u001b[39m: "
     ]
    }
   ],
   "source": [
    "inference_output_dir = 'quantized_inference_results'\n",
    "device = torch.device('cpu')\n",
    "\n",
    "quantized_weights_paths = glob.glob(os.path.join(\n",
    "    'quantized_models', '**', 'quantized_model.pth'), recursive=True)\n",
    "\n",
    "print(\n",
    "    f\"Führe Inferenz für {len(quantized_weights_paths)} quantisierte Modelle aus...\")\n",
    "\n",
    "for weight_path in quantized_weights_paths:\n",
    "    dir_path = os.path.dirname(weight_path)\n",
    "    model_name = Path(weight_path).parent.parent.name\n",
    "    print(f\"Modell aus {model_name} wird verwendet...\")\n",
    "\n",
    "    yaml_filename = None\n",
    "    for file in os.listdir(dir_path):\n",
    "        if file.endswith('.yaml'):\n",
    "            yaml_filename = file\n",
    "            break\n",
    "\n",
    "    if yaml_filename is None:\n",
    "        print(\n",
    "            f\"Keine YAML-Konfigurationsdatei im Verzeichnis {dir_path} gefunden.\")\n",
    "        continue\n",
    "\n",
    "    json_path = os.path.join(dir_path, 'summary_metric.json')\n",
    "    yaml_path = os.path.join(dir_path, yaml_filename)\n",
    "\n",
    "    if not os.path.exists(json_path):\n",
    "        print(\n",
    "            f\"Keine JSON-Zusammenfassungsdatei im Verzeichnis {dir_path} gefunden.\")\n",
    "        continue\n",
    "\n",
    "    config = load_config(yaml_path)\n",
    "    summary_data = load_json(json_path)\n",
    "    training_id = summary_data.get('training_id', 'quantized_run')\n",
    "    print(\"Lade Modelle fuer die Quantisierung...\")\n",
    "    inference_model = STFPM(\n",
    "        architecture=config['model']['architecture'],\n",
    "        layers=config['model']['layers'],\n",
    "        quantize=True\n",
    "    ).to(device).eval()\n",
    "    stem_model = inference_model.stem_model\n",
    "    student_model_to_quantize = inference_model.student_model.to(device).eval()\n",
    "    qconfig_mapping = get_default_qconfig_mapping('fbgemm')\n",
    "    example_inputs = (stem_model(torch.randn(\n",
    "        1, 3, config['dataset']['img_size'], config['dataset']['img_size']\n",
    "    )),)\n",
    "    print(\"Bereite das Studenten-Modell fuer die Quantisierung vor...\")\n",
    "    prepared_model = quantize_fx.prepare_fx(\n",
    "        student_model_to_quantize, qconfig_mapping, example_inputs\n",
    "    )\n",
    "    print(\"Konvertiere das vorbereitete Modell in ein quantisiertes Modell...\")\n",
    "    quantized_student_model = quantize_fx.convert_fx(prepared_model)\n",
    "    print(\"Lade die quantisierten Gewichte in das Modell...\")\n",
    "    quantized_student_model.load_state_dict(\n",
    "        torch.load(weight_path, map_location=device)\n",
    "    )\n",
    "    quantized_student_model.eval()\n",
    "\n",
    "    inference_model.student_model = quantized_student_model.to(device)\n",
    "\n",
    "    try:\n",
    "        test_set = MVTecDataset(\n",
    "            img_size=config['dataset']['img_size'],\n",
    "            base_path=config['dataset']['base_path'],\n",
    "            cls=config['dataset']['class'],\n",
    "            mode='test',\n",
    "            download_if_missing=False\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            test_set,\n",
    "            batch_size=config['dataloader']['batch_size'],\n",
    "            shuffle=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden des Test-Datensatzes für {yaml_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    infer = Inference(\n",
    "        model=inference_model,\n",
    "        test_loader=test_loader,\n",
    "        config=config,\n",
    "        output_dir=inference_output_dir,\n",
    "        path_to_student_weight=None,\n",
    "        trainings_id=training_id,\n",
    "        inferenz=True\n",
    "    )\n",
    "\n",
    "    print(f\"Starte Inferenz für Konfiguration: {yaml_path}...\")\n",
    "    auroc_score, total_inference_time = infer.evaluate_loaded_model()\n",
    "    infer.create_inference_summary(\n",
    "        summary_data, auroc_score, total_inference_time)\n",
    "    print(\n",
    "        f\"Inferenz abgeschlossen. AUROC: {auroc_score:.4f}, Zeit: {total_inference_time:.4f}s.\")\n",
    "\n",
    "    infer.generate_heatmaps_from_saved_maps()\n",
    "\n",
    "print(\"\\n--- Alle Inferenzläufe abgeschlossen. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ad1f74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureExtractor(\n",
      "  (model): QuantizableResNet(\n",
      "    (conv1): Identity()\n",
      "    (bn1): Identity()\n",
      "    (relu): Identity()\n",
      "    (maxpool): Identity()\n",
      "    (layer1): Sequential(\n",
      "      (0): QuantizableBasicBlock(\n",
      "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.016524722799658775, zero_point=0, padding=(1, 1))\n",
      "        (bn1): Identity()\n",
      "        (relu): Identity()\n",
      "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.04645531252026558, zero_point=75, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (add_relu): QFunctional(\n",
      "          scale=0.03447607904672623, zero_point=0\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (1): QuantizableBasicBlock(\n",
      "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.017180869355797768, zero_point=0, padding=(1, 1))\n",
      "        (bn1): Identity()\n",
      "        (relu): Identity()\n",
      "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.06583978235721588, zero_point=82, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (add_relu): QFunctional(\n",
      "          scale=0.03704456984996796, zero_point=0\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): QuantizableBasicBlock(\n",
      "        (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.014848409220576286, zero_point=0, padding=(1, 1))\n",
      "        (bn1): Identity()\n",
      "        (relu): Identity()\n",
      "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.043153878301382065, zero_point=58, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (downsample): Sequential(\n",
      "          (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.03397900238633156, zero_point=68)\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (add_relu): QFunctional(\n",
      "          scale=0.02582652121782303, zero_point=0\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (1): QuantizableBasicBlock(\n",
      "        (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.015309284441173077, zero_point=0, padding=(1, 1))\n",
      "        (bn1): Identity()\n",
      "        (relu): Identity()\n",
      "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.04422105476260185, zero_point=70, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (add_relu): QFunctional(\n",
      "          scale=0.032176367938518524, zero_point=0\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): QuantizableBasicBlock(\n",
      "        (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.018436331301927567, zero_point=0, padding=(1, 1))\n",
      "        (bn1): Identity()\n",
      "        (relu): Identity()\n",
      "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.05117332935333252, zero_point=47, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (downsample): Sequential(\n",
      "          (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.014961435459554195, zero_point=84)\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (add_relu): QFunctional(\n",
      "          scale=0.027315333485603333, zero_point=0\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (1): QuantizableBasicBlock(\n",
      "        (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.01651308871805668, zero_point=0, padding=(1, 1))\n",
      "        (bn1): Identity()\n",
      "        (relu): Identity()\n",
      "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.046813614666461945, zero_point=77, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (add_relu): QFunctional(\n",
      "          scale=0.026840999722480774, zero_point=0\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): QuantizableBasicBlock(\n",
      "        (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.013452869839966297, zero_point=0, padding=(1, 1))\n",
      "        (bn1): Identity()\n",
      "        (relu): Identity()\n",
      "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.04732713848352432, zero_point=68, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (downsample): Sequential(\n",
      "          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.036629535257816315, zero_point=65)\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (add_relu): QFunctional(\n",
      "          scale=0.029616426676511765, zero_point=0\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (1): QuantizableBasicBlock(\n",
      "        (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.013848909176886082, zero_point=0, padding=(1, 1))\n",
      "        (bn1): Identity()\n",
      "        (relu): Identity()\n",
      "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.2509911060333252, zero_point=42, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (add_relu): QFunctional(\n",
      "          scale=0.17645132541656494, zero_point=0\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): QuantizedLinear(in_features=512, out_features=1000, scale=0.2849271297454834, zero_point=35, qscheme=torch.per_channel_affine)\n",
      "    (quant): Quantize(scale=tensor([0.0374]), zero_point=tensor([57]), dtype=torch.quint8)\n",
      "    (dequant): DeQuantize()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "config = load_config(r'Configs\\resnet18\\grid\\STFPM_Config_resnet18_grid.yaml')\n",
    "model = STFPM(\n",
    "    architecture=config['model']['architecture'],\n",
    "    layers=config['model']['layers'],\n",
    "    quantize=True\n",
    ")\n",
    "print(model.teacher_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b66448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import time  # Hinzugefügt: Modul für die Zeitmessung\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. ONNX-Modell laden\n",
    "onnx_model_path = r\"onnx_models\\STFPM_bottle_mobilenetv4_conv_large.onnx\"\n",
    "sess = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "image_path = glob.glob(os.path.join(\n",
    "    r'Images\\bottle\\test\\broken_large', '*.png'))\n",
    "for path in image_path:\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "    img_size = 256\n",
    "    image = image.resize((img_size, img_size))\n",
    "\n",
    "    input_data = np.array(image, dtype=np.uint8)\n",
    "    input_data = np.expand_dims(input_data, axis=0)\n",
    "\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    outputs = sess.run(None, {input_name: input_data})\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    inference_time = end_time - start_time\n",
    "\n",
    "    anomaly_map = outputs[0]\n",
    "    anomaly_score = outputs[1]\n",
    "\n",
    "    print(f\"Anomalie-Score für das Bild: {anomaly_score[0]}\")\n",
    "    print(f\"Inferenzzeit: {inference_time:.4f} Sekunden\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # 2. Originalbild anzeigen\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Originalbild\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # 3. Heatmap über das Originalbild legen\n",
    "    axes[1].imshow(image)\n",
    "    heatmap = axes[1].imshow(np.squeeze(anomaly_map), cmap='jet', alpha=0.5)\n",
    "    axes[1].set_title(\"Anomalie-Heatmap\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # 4. Farbbalken für die Heatmap hinzufügen\n",
    "    fig.colorbar(heatmap, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    # 5. Plot anzeigen oder speichern\n",
    "    plt.suptitle(\"Vergleich: Originalbild vs. Anomalie-Heatmap\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
