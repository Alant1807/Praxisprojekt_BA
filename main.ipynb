{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554194bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#os.environ[\"OMP_NUM_THREADS\"] = str(os.cpu_count() // 2)\n",
    "#os.environ[\"GOMP_CPU_AFFINITY\"] = \"granularity=core,compact\"\n",
    "from Scripts.model import *\n",
    "from Scripts.loss import *\n",
    "from Scripts.results_manager import *\n",
    "from Scripts.plots import *\n",
    "from Scripts.dataset import *\n",
    "from Scripts.trainer import *\n",
    "from Scripts.inference import *\n",
    "from Scripts.Onnx_Class import *\n",
    "from Scripts.lr_finder import *\n",
    "from Scripts.generate_configs import *\n",
    "from Scripts.excecute import *\n",
    "from Scripts.upload_summaries import *\n",
    "from Scripts.quantize import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581e96cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"Configs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6181dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_selected_class(config_path, 'grid')\n",
    "# metrics_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e587851",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_all_classes(config_path)\n",
    "metrics_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d3e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_run_folder = \"Training_Runs\"\n",
    "inference_output_dir = \"Inference_Runs\"\n",
    "\n",
    "inference_model(training_run_folder, inference_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1c54cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"timm_training_runs\"\n",
    "\n",
    "config_paths_all = glob.glob(os.path.join(\n",
    "    base_path, \"**\", \"*.yaml\"), recursive=True)\n",
    "summary_metrics_paths_all = glob.glob(os.path.join(\n",
    "    base_path, \"**\", \"summary_metrics.json\"), recursive=True)\n",
    "best_student_weight_paths_all = glob.glob(os.path.join(\n",
    "    base_path, \"**\", \"*best_student.pth\"), recursive=True)\n",
    "\n",
    "for configs, summary_metrics, best_student_weights in zip(config_paths_all, summary_metrics_paths_all, best_student_weight_paths_all):\n",
    "    config = load_config(configs)\n",
    "    summary_metric = load_json(summary_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef59b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_output_dir = 'quantized_inference_results'\n",
    "device = torch.device('cpu')\n",
    "\n",
    "quantized_weights_paths = glob.glob(os.path.join(\n",
    "    'quantized_models', '**', 'quantized_model.pth'), recursive=True)\n",
    "\n",
    "print(\n",
    "    f\"Führe Inferenz für {len(quantized_weights_paths)} quantisierte Modelle aus...\")\n",
    "\n",
    "for weight_path in quantized_weights_paths:\n",
    "    dir_path = os.path.dirname(weight_path)\n",
    "    model_name = Path(weight_path).parent.parent.name\n",
    "    print(f\"Modell aus {model_name} wird verwendet...\")\n",
    "\n",
    "    yaml_filename = None\n",
    "    for file in os.listdir(dir_path):\n",
    "        if file.endswith('.yaml'):\n",
    "            yaml_filename = file\n",
    "            break\n",
    "\n",
    "    if yaml_filename is None:\n",
    "        print(\n",
    "            f\"Keine YAML-Konfigurationsdatei im Verzeichnis {dir_path} gefunden.\")\n",
    "        continue\n",
    "\n",
    "    json_path = os.path.join(dir_path, 'summary_metric.json')\n",
    "    yaml_path = os.path.join(dir_path, yaml_filename)\n",
    "\n",
    "    if not os.path.exists(json_path):\n",
    "        print(\n",
    "            f\"Keine JSON-Zusammenfassungsdatei im Verzeichnis {dir_path} gefunden.\")\n",
    "        continue\n",
    "\n",
    "    config = load_config(yaml_path)\n",
    "    summary_data = load_json(json_path)\n",
    "    training_id = summary_data.get('training_id', 'quantized_run')\n",
    "    print(\"Lade Modelle fuer die Quantisierung...\")\n",
    "    inference_model = STFPM(\n",
    "        architecture=config['model']['architecture'],\n",
    "        layers=config['model']['layers'],\n",
    "        quantize=False\n",
    "    ).to(device).eval()\n",
    "    stem_model = inference_model.stem_model\n",
    "    student_model_to_quantize = inference_model.student_model.to(device).eval()\n",
    "    qconfig_mapping = get_default_qconfig_mapping('fbgemm')\n",
    "    example_inputs = (stem_model(torch.randn(\n",
    "        1, 3, config['dataset']['img_size'], config['dataset']['img_size']\n",
    "    )),)\n",
    "    print(\"Bereite das Studenten-Modell fuer die Quantisierung vor...\")\n",
    "    prepared_model = quantize_fx.prepare_fx(\n",
    "        student_model_to_quantize, qconfig_mapping, example_inputs\n",
    "    )\n",
    "    print(\"Konvertiere das vorbereitete Modell in ein quantisiertes Modell...\")\n",
    "    quantized_student_model = quantize_fx.convert_fx(prepared_model)\n",
    "    print(\"Lade die quantisierten Gewichte in das Modell...\")\n",
    "    quantized_student_model.load_state_dict(\n",
    "        torch.load(weight_path, map_location=device)\n",
    "    )\n",
    "    quantized_student_model.eval()\n",
    "\n",
    "    inference_model.student_model = quantized_student_model.to(device)\n",
    "\n",
    "    try:\n",
    "        test_set = MVTecDataset(\n",
    "            img_size=config['dataset']['img_size'],\n",
    "            base_path=config['dataset']['base_path'],\n",
    "            cls=config['dataset']['class'],\n",
    "            mode='test',\n",
    "            download_if_missing=False\n",
    "        )\n",
    "        print(\"Lade den gesamten Test-Datensatz in den Arbeitsspeicher...\")\n",
    "        memory_cache = [test_set[i] for i in tqdm(range(len(test_set)))]\n",
    "        test_loader = DataLoader(\n",
    "            memory_cache,\n",
    "            batch_size=config['dataloader']['batch_size'],\n",
    "            shuffle=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden des Test-Datensatzes für {yaml_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    infer = Inference(\n",
    "        model=inference_model,\n",
    "        test_loader=test_loader,\n",
    "        config=config,\n",
    "        output_dir=inference_output_dir,\n",
    "        path_to_student_weight=None,\n",
    "        trainings_id=training_id,\n",
    "        inferenz=True\n",
    "    )\n",
    "\n",
    "    print(f\"Starte Inferenz für Konfiguration: {yaml_path}...\")\n",
    "    auroc_score, total_inference_time = infer.evaluate_quantized_loaded_model()\n",
    "    infer.create_inference_summary(\n",
    "        summary_data, auroc_score, total_inference_time)\n",
    "    print(\n",
    "        f\"Inferenz abgeschlossen. AUROC: {auroc_score:.4f}, Zeit: {total_inference_time:.4f}s.\")\n",
    "\n",
    "    # infer.generate_heatmaps_from_saved_maps()\n",
    "\n",
    "print(\"\\n--- Alle Inferenzläufe abgeschlossen. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b66448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import time  # Hinzugefügt: Modul für die Zeitmessung\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. ONNX-Modell laden\n",
    "onnx_model_path = r\"onnx_models\\STFPM_bottle_mobilenetv4_conv_large.onnx\"\n",
    "sess = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "image_path = glob.glob(os.path.join(\n",
    "    r'Images\\bottle\\test\\broken_large', '*.png'))\n",
    "for path in image_path:\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "    img_size = 256\n",
    "    image = image.resize((img_size, img_size))\n",
    "\n",
    "    input_data = np.array(image, dtype=np.uint8)\n",
    "    input_data = np.expand_dims(input_data, axis=0)\n",
    "\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    outputs = sess.run(None, {input_name: input_data})\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    inference_time = end_time - start_time\n",
    "\n",
    "    anomaly_map = outputs[0]\n",
    "    anomaly_score = outputs[1]\n",
    "\n",
    "    print(f\"Anomalie-Score für das Bild: {anomaly_score[0]}\")\n",
    "    print(f\"Inferenzzeit: {inference_time:.4f} Sekunden\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # 2. Originalbild anzeigen\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Originalbild\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # 3. Heatmap über das Originalbild legen\n",
    "    axes[1].imshow(image)\n",
    "    heatmap = axes[1].imshow(np.squeeze(anomaly_map), cmap='jet', alpha=0.5)\n",
    "    axes[1].set_title(\"Anomalie-Heatmap\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # 4. Farbbalken für die Heatmap hinzufügen\n",
    "    fig.colorbar(heatmap, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    # 5. Plot anzeigen oder speichern\n",
    "    plt.suptitle(\"Vergleich: Originalbild vs. Anomalie-Heatmap\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
