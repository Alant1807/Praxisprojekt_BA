{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "554194bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openvino\\runtime\\__init__.py:10: DeprecationWarning: The `openvino.runtime` module is deprecated and will be removed in the 2026.0 release. Please replace `openvino.runtime` with `openvino`.\n",
      "  warnings.warn(\n",
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
      "W0828 16:05:31.935000 2420 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#os.environ[\"OMP_NUM_THREADS\"] = str(os.cpu_count() // 2)\n",
    "#os.environ[\"GOMP_CPU_AFFINITY\"] = \"granularity=core,compact\"\n",
    "from Scripts.model2 import *\n",
    "from Scripts.loss import *\n",
    "from Scripts.results_manager import *\n",
    "from Scripts.plots import *\n",
    "from Scripts.dataset import *\n",
    "from Scripts.trainer import *\n",
    "from Scripts.inference import *\n",
    "from Scripts.Onnx_Class import *\n",
    "from Scripts.lr_finder import *\n",
    "from Scripts.generate_configs import *\n",
    "from Scripts.excecute import *\n",
    "from Scripts.upload_summaries import *\n",
    "from Scripts.quantize import *\n",
    "from Scripts.quantize_pt2e import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581e96cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"Configs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6181dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_selected_class(config_path, 'grid')\n",
    "# metrics_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e587851",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_all_classes(config_path)\n",
    "metrics_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d3e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_run_folder = \"Training_Runs\"\n",
    "inference_output_dir = \"Inference_Runs\"\n",
    "\n",
    "inference_model(training_run_folder, inference_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c1c54cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Node arity mismatch; expected 0, but got 3.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m config = load_config(configs)\n\u001b[32m     10\u001b[39m summary_metric = load_json(summary_metrics)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mquantize_model_pt2e\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbest_student_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43msummary_metric\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\Praxisprojekt_BA\\Scripts\\quantize_pt2e.py:34\u001b[39m, in \u001b[36mquantize_model_pt2e\u001b[39m\u001b[34m(best_weights_path, config, summary_metric)\u001b[39m\n\u001b[32m     31\u001b[39m example_inputs = (stem_model(torch.randn(\n\u001b[32m     32\u001b[39m     \u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m256\u001b[39m, \u001b[32m256\u001b[39m).contiguous(memory_format=torch.channels_last)),)\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     exported_model = \u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudent_to_quantize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m quantizer = X86InductorQuantizer()\n\u001b[32m     39\u001b[39m quantizer.set_global(xiq.get_default_x86_inductor_quantization_config())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\export\\__init__.py:319\u001b[39m, in \u001b[36mexport\u001b[39m\u001b[34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature)\u001b[39m\n\u001b[32m    317\u001b[39m     new_msg = \u001b[38;5;28mstr\u001b[39m(e) + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + draft_export_msg\n\u001b[32m    318\u001b[39m     e.args = (new_msg,)\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\export\\__init__.py:286\u001b[39m, in \u001b[36mexport\u001b[39m\u001b[34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature)\u001b[39m\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    280\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExporting a ScriptModule is not supported. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    281\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMaybe try converting your ScriptModule to an ExportedProgram \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    282\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33musing `TS2EPConverter(mod, args, kwargs).convert()` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    283\u001b[39m     )\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    296\u001b[39m     draft_export_msg = (\n\u001b[32m    297\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe error above occurred when calling torch.export.export. If you would \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    298\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlike to view some more information about this error, and get a list \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    299\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mof all other errors that may occur in your export call, you can \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    300\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mreplace your `export()` call with `draft_export()`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    301\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\export\\_trace.py:1164\u001b[39m, in \u001b[36m_log_export_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1158\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mpartial_fx_graph\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1159\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m   1160\u001b[39m             e.partial_fx_graph,\n\u001b[32m   1161\u001b[39m             file=sys.stderr,\n\u001b[32m   1162\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1164\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     _EXPORT_FLAGS = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\export\\_trace.py:1130\u001b[39m, in \u001b[36m_log_export_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1128\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1129\u001b[39m     start = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m     ep = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1131\u001b[39m     end = time.time()\n\u001b[32m   1132\u001b[39m     log_export_usage(\n\u001b[32m   1133\u001b[39m         event=\u001b[33m\"\u001b[39m\u001b[33mexport.time\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1134\u001b[39m         metrics=end - start,\n\u001b[32m   1135\u001b[39m         flags=_EXPORT_FLAGS,\n\u001b[32m   1136\u001b[39m         **get_ep_stats(ep),\n\u001b[32m   1137\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\export\\exported_program.py:123\u001b[39m, in \u001b[36m_disable_prexisiting_fake_mode.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(fn)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m unset_fake_temporarily():\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\export\\_trace.py:2176\u001b[39m, in \u001b[36m_export\u001b[39m\u001b[34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature, pre_dispatch, allow_complex_guards_as_runtime_asserts, _is_torch_jit_trace)\u001b[39m\n\u001b[32m   2168\u001b[39m \u001b[38;5;66;03m# NOTE Export training IR rollout\u001b[39;00m\n\u001b[32m   2169\u001b[39m \u001b[38;5;66;03m# Old export calls export._trace(pre_dispatch=True)\u001b[39;00m\n\u001b[32m   2170\u001b[39m \u001b[38;5;66;03m# and there are still lot of internal/OSS callsites that\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2173\u001b[39m \u001b[38;5;66;03m# export_training_ir_rollout_check returns True in OSS\u001b[39;00m\n\u001b[32m   2174\u001b[39m \u001b[38;5;66;03m# while internally it returns False UNLESS otherwise specified.\u001b[39;00m\n\u001b[32m   2175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;129;01mand\u001b[39;00m export_training_ir_rollout_check():\n\u001b[32m-> \u001b[39m\u001b[32m2176\u001b[39m     ep = \u001b[43m_export_for_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2178\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2179\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2180\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2181\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2182\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2183\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2184\u001b[39m     dtrace_structured(\u001b[33m\"\u001b[39m\u001b[33mexported_program\u001b[39m\u001b[33m\"\u001b[39m, payload_fn=\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mstr\u001b[39m(ep))\n\u001b[32m   2185\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ep\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\export\\_trace.py:1164\u001b[39m, in \u001b[36m_log_export_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1158\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mpartial_fx_graph\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1159\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m   1160\u001b[39m             e.partial_fx_graph,\n\u001b[32m   1161\u001b[39m             file=sys.stderr,\n\u001b[32m   1162\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1164\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     _EXPORT_FLAGS = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\export\\_trace.py:1130\u001b[39m, in \u001b[36m_log_export_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1128\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1129\u001b[39m     start = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m     ep = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1131\u001b[39m     end = time.time()\n\u001b[32m   1132\u001b[39m     log_export_usage(\n\u001b[32m   1133\u001b[39m         event=\u001b[33m\"\u001b[39m\u001b[33mexport.time\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1134\u001b[39m         metrics=end - start,\n\u001b[32m   1135\u001b[39m         flags=_EXPORT_FLAGS,\n\u001b[32m   1136\u001b[39m         **get_ep_stats(ep),\n\u001b[32m   1137\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\export\\exported_program.py:123\u001b[39m, in \u001b[36m_disable_prexisiting_fake_mode.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(fn)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m unset_fake_temporarily():\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\export\\_trace.py:2037\u001b[39m, in \u001b[36m_export_for_training\u001b[39m\u001b[34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature)\u001b[39m\n\u001b[32m   2034\u001b[39m \u001b[38;5;66;03m# Call the appropriate export function based on the strictness of tracing.\u001b[39;00m\n\u001b[32m   2035\u001b[39m export_func = _strict_export \u001b[38;5;28;01mif\u001b[39;00m strict \u001b[38;5;28;01melse\u001b[39;00m _non_strict_export\n\u001b[32m-> \u001b[39m\u001b[32m2037\u001b[39m export_artifact = \u001b[43mexport_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2038\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2039\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2040\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2041\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2042\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreserve_module_call_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2043\u001b[39m \u001b[43m    \u001b[49m\u001b[43morig_in_spec\u001b[49m\u001b[43m=\u001b[49m\u001b[43morig_in_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2044\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_complex_guards_as_runtime_asserts\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2045\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_torch_jit_trace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2046\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_to_aten_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_export_to_aten_ir_make_fx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2047\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2049\u001b[39m export_graph_signature = export_artifact.aten.sig\n\u001b[32m   2051\u001b[39m forward_arg_names = _get_forward_arg_names(mod, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\export\\_trace.py:1979\u001b[39m, in \u001b[36m_non_strict_export\u001b[39m\u001b[34m(mod, args, kwargs, dynamic_shapes, preserve_module_call_signature, orig_in_spec, allow_complex_guards_as_runtime_asserts, _is_torch_jit_trace, _to_aten_func)\u001b[39m\n\u001b[32m   1962\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m   1963\u001b[39m     fake_mode,\n\u001b[32m   1964\u001b[39m     _NonStrictTorchFunctionHandler(),\n\u001b[32m   1965\u001b[39m     tracing(tx),\n\u001b[32m   1966\u001b[39m     torch._dynamo.config.patch(dynamo_config),\n\u001b[32m   1967\u001b[39m ):\n\u001b[32m   1968\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m   1969\u001b[39m         _fakify_script_objects(mod, fake_args, fake_kwargs, fake_mode) \u001b[38;5;28;01mas\u001b[39;00m (\n\u001b[32m   1970\u001b[39m             patched_mod,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1977\u001b[39m         _override_builtin_ops(),\n\u001b[32m   1978\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1979\u001b[39m         aten_export_artifact = \u001b[43m_to_aten_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[operator]\u001b[39;49;00m\n\u001b[32m   1980\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpatched_mod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1981\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnew_fake_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1982\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnew_fake_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1983\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfake_params_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1984\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnew_fake_constant_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1985\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproduce_guards_callback\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_produce_guards_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1986\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_tuplify_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1987\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m         \u001b[38;5;66;03m# aten_export_artifact.constants contains only fake script objects, we need to map them back\u001b[39;00m\n\u001b[32m   1989\u001b[39m         aten_export_artifact.constants = {\n\u001b[32m   1990\u001b[39m             fqn: map_fake_to_real[obj] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, FakeScriptObject) \u001b[38;5;28;01melse\u001b[39;00m obj\n\u001b[32m   1991\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m fqn, obj \u001b[38;5;129;01min\u001b[39;00m aten_export_artifact.constants.items()\n\u001b[32m   1992\u001b[39m         }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\export\\_trace.py:1770\u001b[39m, in \u001b[36m_export_to_aten_ir_make_fx\u001b[39m\u001b[34m(mod, fake_args, fake_kwargs, fake_params_buffers, constant_attrs, produce_guards_callback, transform)\u001b[39m\n\u001b[32m   1756\u001b[39m \u001b[38;5;66;03m# This _reparametrize_module makes sure inputs and module.params/buffers have the same fake_mode,\u001b[39;00m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# otherwise aot_export_module will error out because it sees a mix of fake_modes.\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# And we want aot_export_module to use the fake_tensor mode in dynamo to keep the pipeline easy to reason about.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m   1760\u001b[39m     torch.nn.utils.stateless._reparametrize_module(\n\u001b[32m   1761\u001b[39m         mod,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1768\u001b[39m     _compiling_state_context(),\n\u001b[32m   1769\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1770\u001b[39m     gm, graph_signature = \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_make_fx_helper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1771\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1772\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfake_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1773\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrace_joint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1774\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfake_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1775\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1777\u001b[39m     \u001b[38;5;66;03m# [NOTE] In training IR, we don't run\u001b[39;00m\n\u001b[32m   1778\u001b[39m     \u001b[38;5;66;03m# any DCE as a result we preserve constant\u001b[39;00m\n\u001b[32m   1779\u001b[39m     \u001b[38;5;66;03m# nodes in the graph. make_fx invariant is that\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1782\u001b[39m     \u001b[38;5;66;03m# graph, the node.meta here actually doesn't matter. But\u001b[39;00m\n\u001b[32m   1783\u001b[39m     \u001b[38;5;66;03m# we do this to make spec verifier happy.\u001b[39;00m\n\u001b[32m   1784\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m gm.graph.nodes:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\export\\_trace.py:1900\u001b[39m, in \u001b[36m_non_strict_export.<locals>._tuplify_outputs.<locals>._aot_export_non_strict\u001b[39m\u001b[34m(mod, args, kwargs, **flags)\u001b[39m\n\u001b[32m   1896\u001b[39m     ctx = _wrap_submodules(  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m   1897\u001b[39m         wrapped_mod, new_preserved_call_signatures, module_call_specs\n\u001b[32m   1898\u001b[39m     )\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[32m-> \u001b[39m\u001b[32m1900\u001b[39m     gm, sig = \u001b[43maot_export\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped_mod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1901\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mExported program from AOTAutograd:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, gm)\n\u001b[32m   1903\u001b[39m sig.parameters = pytree.tree_map(_strip_root, sig.parameters)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\export\\_trace.py:1684\u001b[39m, in \u001b[36m_export_to_aten_ir_make_fx.<locals>._make_fx_helper\u001b[39m\u001b[34m(mod, args, kwargs, **flags)\u001b[39m\n\u001b[32m   1681\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, (old_getattr, _) \u001b[38;5;129;01min\u001b[39;00m tensor_type_to_old_getattribute.items():\n\u001b[32m   1682\u001b[39m             k.\u001b[34m__getattribute__\u001b[39m = old_getattr  \u001b[38;5;66;03m# type: ignore[method-assign, attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1684\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverride_getattribute_for_subclasses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake_fx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1686\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwrapped_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1687\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrecord_module_stack\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1688\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m non_strict_root \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py:148\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    150\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py:1718\u001b[39m, in \u001b[36m_detect_attribute_assignment\u001b[39m\u001b[34m(mod)\u001b[39m\n\u001b[32m   1710\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(deleted_attrs) > \u001b[32m0\u001b[39m:\n\u001b[32m   1711\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1712\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDuring torch.export, following attrs were deleted in the model.forward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeleted_attrs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1713\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSuch attributes must be registered as buffers using the `register_buffer` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1714\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAPI and must be initialized at model.__init__ \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1715\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1716\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1718\u001b[39m \u001b[43mpytree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtree_map_with_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1719\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_collect_assigned_tensor_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_attrs\u001b[49m\n\u001b[32m   1720\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1721\u001b[39m \u001b[38;5;66;03m# restore state of all attributes (including, e.g., of primitive types)\u001b[39;00m\n\u001b[32m   1722\u001b[39m mod.\u001b[34m__dict__\u001b[39m.update(snapshot)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\_pytree.py:2076\u001b[39m, in \u001b[36mtree_map_with_path\u001b[39m\u001b[34m(func, tree, is_leaf, *rests)\u001b[39m\n\u001b[32m   2074\u001b[39m keypath_leaves, treespec = tree_flatten_with_path(tree, is_leaf)\n\u001b[32m   2075\u001b[39m keypath_leaves = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*keypath_leaves))\n\u001b[32m-> \u001b[39m\u001b[32m2076\u001b[39m all_keypath_leaves = keypath_leaves + [\u001b[43mtreespec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflatten_up_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[32m   2077\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m treespec.unflatten(func(*xs) \u001b[38;5;28;01mfor\u001b[39;00m xs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(*all_keypath_leaves))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\_pytree.py:1192\u001b[39m, in \u001b[36mTreeSpec.flatten_up_to\u001b[39m\u001b[34m(self, tree)\u001b[39m\n\u001b[32m   1189\u001b[39m         helper(subspec, subtree, subtrees)\n\u001b[32m   1191\u001b[39m subtrees: \u001b[38;5;28mlist\u001b[39m[PyTree] = []\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m \u001b[43mhelper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m subtrees\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\_pytree.py:1189\u001b[39m, in \u001b[36mTreeSpec.flatten_up_to.<locals>.helper\u001b[39m\u001b[34m(treespec, tree, subtrees)\u001b[39m\n\u001b[32m   1183\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1184\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNode context mismatch for node type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtreespec.type\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1185\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mexpected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtreespec.context\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# namedtuple type mismatch\u001b[39;00m\n\u001b[32m   1186\u001b[39m             )\n\u001b[32m   1188\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m subtree, subspec \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(children, treespec.children_specs):\n\u001b[32m-> \u001b[39m\u001b[32m1189\u001b[39m     \u001b[43mhelper\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubtrees\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\_pytree.py:1150\u001b[39m, in \u001b[36mTreeSpec.flatten_up_to.<locals>.helper\u001b[39m\u001b[34m(treespec, tree, subtrees)\u001b[39m\n\u001b[32m   1145\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1146\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNode type mismatch; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1147\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mexpected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtreespec.type\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_type\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1148\u001b[39m     )\n\u001b[32m   1149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tree) != treespec.num_children:\n\u001b[32m-> \u001b[39m\u001b[32m1150\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1151\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNode arity mismatch; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1152\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mexpected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtreespec.num_children\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tree)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1153\u001b[39m     )\n\u001b[32m   1155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m both_standard_dict:\n\u001b[32m   1156\u001b[39m     \u001b[38;5;66;03m# dictionary types are compatible with each other\u001b[39;00m\n\u001b[32m   1157\u001b[39m     dict_context = (\n\u001b[32m   1158\u001b[39m         treespec.context\n\u001b[32m   1159\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m treespec.type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m defaultdict\n\u001b[32m   1160\u001b[39m         \u001b[38;5;66;03m# ignore mismatch of `default_factory` for defaultdict\u001b[39;00m\n\u001b[32m   1161\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m treespec.context[\u001b[32m1\u001b[39m]\n\u001b[32m   1162\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Node arity mismatch; expected 0, but got 3."
     ]
    }
   ],
   "source": [
    "config_paths_all = glob.glob(os.path.join(\n",
    "    \"Training_Runs\", \"**\", \"*.yaml\"), recursive=True)\n",
    "summary_metrics_paths_all = glob.glob(os.path.join(\n",
    "    \"Training_Runs\", \"**\", \"summary_metrics.json\"), recursive=True)\n",
    "best_student_weight_paths_all = glob.glob(os.path.join(\n",
    "    \"Training_Runs\", \"**\", \"*best_student.pth\"), recursive=True)\n",
    "\n",
    "for configs, summary_metrics, best_student_weights in zip(config_paths_all, summary_metrics_paths_all, best_student_weight_paths_all):\n",
    "    config = load_config(configs)\n",
    "    summary_metric = load_json(summary_metrics)\n",
    "    quantize_model_pt2e(\n",
    "        best_student_weights,\n",
    "        config,\n",
    "        summary_metric\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ef59b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Führe Inferenz für 3 quantisierte Modelle aus...\n",
      "Modell aus mobilenet_v3_large wird verwendet...\n",
      "Lade Modelle fuer die Quantisierung...\n",
      "Bereite das Studenten-Modell fuer die Quantisierung vor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alant\\AppData\\Local\\Temp\\ipykernel_14956\\2731857662.py:50: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  prepared_model = quantize_fx.prepare_fx(\n",
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konvertiere das vorbereitete Modell in ein quantisiertes Modell...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alant\\AppData\\Local\\Temp\\ipykernel_14956\\2731857662.py:54: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  quantized_student_model = quantize_fx.convert_fx(prepared_model)\n",
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:1343: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n",
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_utils.py:465: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade die quantisierten Gewichte in das Modell...\n",
      "Lade den gesamten Test-Datensatz in den Arbeitsspeicher...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 65.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Inferenz für Konfiguration: quantized_models\\MVTecAD_grid\\mobilenet_v3_large\\40cc0207-55f9-45d3-b20b-1cc5c5d80040\\STFPM_Config_mobilenet_v3_large.yaml...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PyTorch Profiler-Analyse ---\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                  anomaly_map_inference         6.42%      44.691ms        98.08%     682.612ms     341.306ms             2  \n",
      "                                           aten::conv2d         0.09%     611.600us        27.77%     193.255ms       1.559ms           124  \n",
      "                                      aten::convolution         0.10%     717.800us        27.68%     192.643ms       1.554ms           124  \n",
      "                                     aten::_convolution         0.16%       1.110ms        27.58%     191.925ms       1.548ms           124  \n",
      "                               aten::mkldnn_convolution        27.10%     188.602ms        27.42%     190.815ms       1.539ms           124  \n",
      "                                       aten::batch_norm         0.05%     357.500us        16.84%     117.186ms       1.274ms            92  \n",
      "                           aten::_batch_norm_impl_index         0.11%     789.200us        16.79%     116.828ms       1.270ms            92  \n",
      "                                aten::native_batch_norm        16.23%     112.987ms        16.65%     115.860ms       1.259ms            92  \n",
      "                                 quantized::conv2d_relu        11.11%      77.295ms        11.15%      77.593ms       2.425ms            32  \n",
      "                                        aten::clamp_min         7.85%      54.616ms         7.85%      54.616ms       1.092ms            50  \n",
      "                                             aten::relu         0.02%     173.000us         7.83%      54.503ms       1.434ms            38  \n",
      "                                      quantized::conv2d         6.88%      47.891ms         7.10%      49.434ms     852.305us            58  \n",
      "                              aten::adaptive_avg_pool2d         0.03%     190.300us         3.40%      23.666ms     845.196us            28  \n",
      "                              aten::upsample_bilinear2d         2.95%      20.530ms         2.96%      20.579ms       3.430ms             6  \n",
      "                                         quantized::mul         2.82%      19.601ms         2.82%      19.659ms       1.966ms            10  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 695.978ms\n",
      "\n",
      "Inference summary saved to quantized_inference_results\\MVTecAD_grid\\mobilenet_v3_large\\40cc0207-55f9-45d3-b20b-1cc5c5d80040\\inference_summary.json\n",
      "Inferenz abgeschlossen. AUROC: 0.9081, Zeit: 0.6827s.\n",
      "Modell aus resnet18 wird verwendet...\n",
      "Lade Modelle fuer die Quantisierung...\n",
      "Bereite das Studenten-Modell fuer die Quantisierung vor...\n",
      "Konvertiere das vorbereitete Modell in ein quantisiertes Modell...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alant\\AppData\\Local\\Temp\\ipykernel_14956\\2731857662.py:50: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  prepared_model = quantize_fx.prepare_fx(\n",
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "C:\\Users\\alant\\AppData\\Local\\Temp\\ipykernel_14956\\2731857662.py:54: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  quantized_student_model = quantize_fx.convert_fx(prepared_model)\n",
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:1343: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade die quantisierten Gewichte in das Modell...\n",
      "Lade den gesamten Test-Datensatz in den Arbeitsspeicher...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 64.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Inferenz für Konfiguration: quantized_models\\MVTecAD_grid\\resnet18\\7804c936-9a41-467a-8dfc-e31f5c13998b\\STFPM_Config_resnet18.yaml...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alant\\AppData\\Local\\Temp\\ipykernel_14956\\2731857662.py:50: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  prepared_model = quantize_fx.prepare_fx(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PyTorch Profiler-Analyse ---\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                  anomaly_map_inference         5.37%      78.123ms        99.09%        1.440s     720.168ms             2  \n",
      "                                           aten::conv2d         0.02%     223.800us        48.10%     699.201ms      17.480ms            40  \n",
      "                                      aten::convolution         0.02%     307.200us        48.09%     698.977ms      17.474ms            40  \n",
      "                                     aten::_convolution         0.03%     398.400us        48.06%     698.670ms      17.467ms            40  \n",
      "                               aten::mkldnn_convolution        48.01%     697.931ms        48.04%     698.272ms      17.457ms            40  \n",
      "                                      quantized::conv2d        15.02%     218.268ms        15.07%     218.995ms       9.954ms            22  \n",
      "                                 quantized::conv2d_relu        11.86%     172.383ms        11.88%     172.632ms      10.790ms            16  \n",
      "                                       aten::batch_norm         0.01%     178.900us         4.10%      59.590ms       1.490ms            40  \n",
      "                           aten::_batch_norm_impl_index         0.03%     408.700us         4.09%      59.411ms       1.485ms            40  \n",
      "                                aten::native_batch_norm         4.00%      58.134ms         4.05%      58.903ms       1.473ms            40  \n",
      "                                             aten::relu         0.03%     391.500us         3.83%      55.745ms       1.640ms            34  \n",
      "                                        aten::clamp_min         3.83%      55.678ms         3.83%      55.678ms       1.114ms            50  \n",
      "                              aten::upsample_bilinear2d         1.97%      28.682ms         1.98%      28.760ms       3.595ms             8  \n",
      "                                       aten::max_pool2d         0.00%      29.400us         1.54%      22.365ms      11.182ms             2  \n",
      "                          aten::max_pool2d_with_indices         1.54%      22.335ms         1.54%      22.335ms      11.168ms             2  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.454s\n",
      "\n",
      "Inference summary saved to quantized_inference_results\\MVTecAD_grid\\resnet18\\7804c936-9a41-467a-8dfc-e31f5c13998b\\inference_summary.json\n",
      "Inferenz abgeschlossen. AUROC: 0.9916, Zeit: 1.4404s.\n",
      "Modell aus shufflenet_v2_x1_0 wird verwendet...\n",
      "Lade Modelle fuer die Quantisierung...\n",
      "Bereite das Studenten-Modell fuer die Quantisierung vor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konvertiere das vorbereitete Modell in ein quantisiertes Modell...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alant\\AppData\\Local\\Temp\\ipykernel_14956\\2731857662.py:54: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  quantized_student_model = quantize_fx.convert_fx(prepared_model)\n",
      "c:\\Users\\alant\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:1343: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade die quantisierten Gewichte in das Modell...\n",
      "Lade den gesamten Test-Datensatz in den Arbeitsspeicher...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 64.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Inferenz für Konfiguration: quantized_models\\MVTecAD_grid\\shufflenet_v2_x1_0\\8f25c698-9597-4255-999c-51f15eea019f\\STFPM_Config_shufflenet_v2_x1_0.yaml...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PyTorch Profiler-Analyse ---\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                  anomaly_map_inference         4.73%      60.921ms        99.01%        1.274s     637.107ms             2  \n",
      "                                      quantized::conv2d        63.86%     821.812ms        64.21%     826.306ms      21.745ms            38  \n",
      "                                           aten::conv2d         0.04%     461.900us         9.64%     124.075ms       1.108ms           112  \n",
      "                                      aten::convolution         0.07%     836.800us         9.61%     123.613ms       1.104ms           112  \n",
      "                                     aten::_convolution         0.11%       1.438ms         9.54%     122.776ms       1.096ms           112  \n",
      "                               aten::mkldnn_convolution         7.39%      95.081ms         7.47%      96.076ms     857.821us           112  \n",
      "                                       aten::contiguous         0.03%     425.500us         4.80%      61.828ms     332.406us           186  \n",
      "                                            aten::clone         1.89%      24.381ms         4.77%      61.402ms     330.119us           186  \n",
      "                                 quantized::conv2d_relu         3.32%      42.730ms         4.54%      58.374ms     833.913us            70  \n",
      "                                              aten::cat         2.43%      31.304ms         4.35%      56.010ms     608.804us            92  \n",
      "                                            aten::copy_         3.24%      41.706ms         3.24%      41.706ms     315.953us           132  \n",
      "                                       aten::batch_norm         0.05%     595.700us         3.10%      39.885ms     356.112us           112  \n",
      "                           aten::_batch_norm_impl_index         0.07%     880.000us         3.05%      39.289ms     350.794us           112  \n",
      "                                aten::native_batch_norm         2.80%      35.982ms         2.97%      38.227ms     341.317us           112  \n",
      "                               aten::linalg_vector_norm         1.87%      24.065ms         1.87%      24.065ms       2.005ms            12  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.287s\n",
      "\n",
      "Inference summary saved to quantized_inference_results\\MVTecAD_grid\\shufflenet_v2_x1_0\\8f25c698-9597-4255-999c-51f15eea019f\\inference_summary.json\n",
      "Inferenz abgeschlossen. AUROC: 0.6249, Zeit: 1.2743s.\n",
      "\n",
      "--- Alle Inferenzläufe abgeschlossen. ---\n"
     ]
    }
   ],
   "source": [
    "inference_output_dir = 'quantized_inference_results'\n",
    "device = torch.device('cpu')\n",
    "\n",
    "quantized_weights_paths = glob.glob(os.path.join(\n",
    "    'quantized_models', '**', 'quantized_model.pth'), recursive=True)\n",
    "\n",
    "print(\n",
    "    f\"Führe Inferenz für {len(quantized_weights_paths)} quantisierte Modelle aus...\")\n",
    "\n",
    "for weight_path in quantized_weights_paths:\n",
    "    dir_path = os.path.dirname(weight_path)\n",
    "    model_name = Path(weight_path).parent.parent.name\n",
    "    print(f\"Modell aus {model_name} wird verwendet...\")\n",
    "\n",
    "    yaml_filename = None\n",
    "    for file in os.listdir(dir_path):\n",
    "        if file.endswith('.yaml'):\n",
    "            yaml_filename = file\n",
    "            break\n",
    "\n",
    "    if yaml_filename is None:\n",
    "        print(\n",
    "            f\"Keine YAML-Konfigurationsdatei im Verzeichnis {dir_path} gefunden.\")\n",
    "        continue\n",
    "\n",
    "    json_path = os.path.join(dir_path, 'summary_metric.json')\n",
    "    yaml_path = os.path.join(dir_path, yaml_filename)\n",
    "\n",
    "    if not os.path.exists(json_path):\n",
    "        print(\n",
    "            f\"Keine JSON-Zusammenfassungsdatei im Verzeichnis {dir_path} gefunden.\")\n",
    "        continue\n",
    "\n",
    "    config = load_config(yaml_path)\n",
    "    summary_data = load_json(json_path)\n",
    "    training_id = summary_data.get('training_id', 'quantized_run')\n",
    "    print(\"Lade Modelle fuer die Quantisierung...\")\n",
    "    inference_model = STFPM(\n",
    "        architecture=config['model']['architecture'],\n",
    "        layers=config['model']['layers'],\n",
    "        quantize=False\n",
    "    ).to(device).eval()\n",
    "    stem_model = inference_model.stem_model\n",
    "    student_model_to_quantize = inference_model.student_model.to(device).eval()\n",
    "    qconfig_mapping = get_default_qconfig_mapping('fbgemm')\n",
    "    example_inputs = (stem_model(torch.randn(\n",
    "        1, 3, config['dataset']['img_size'], config['dataset']['img_size']\n",
    "    )),)\n",
    "    print(\"Bereite das Studenten-Modell fuer die Quantisierung vor...\")\n",
    "    prepared_model = quantize_fx.prepare_fx(\n",
    "        student_model_to_quantize, qconfig_mapping, example_inputs\n",
    "    )\n",
    "    print(\"Konvertiere das vorbereitete Modell in ein quantisiertes Modell...\")\n",
    "    quantized_student_model = quantize_fx.convert_fx(prepared_model)\n",
    "    print(\"Lade die quantisierten Gewichte in das Modell...\")\n",
    "    quantized_student_model.load_state_dict(\n",
    "        torch.load(weight_path, map_location=device)\n",
    "    )\n",
    "    quantized_student_model.eval()\n",
    "\n",
    "    inference_model.student_model = quantized_student_model.to(device)\n",
    "\n",
    "    try:\n",
    "        test_set = MVTecDataset(\n",
    "            img_size=config['dataset']['img_size'],\n",
    "            base_path=config['dataset']['base_path'],\n",
    "            cls=config['dataset']['class'],\n",
    "            mode='test',\n",
    "            download_if_missing=False\n",
    "        )\n",
    "        print(\"Lade den gesamten Test-Datensatz in den Arbeitsspeicher...\")\n",
    "        memory_cache = [test_set[i] for i in tqdm(range(len(test_set)))]\n",
    "        test_loader = DataLoader(\n",
    "            memory_cache,\n",
    "            batch_size=config['dataloader']['batch_size'],\n",
    "            shuffle=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden des Test-Datensatzes für {yaml_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    infer = Inference(\n",
    "        model=inference_model,\n",
    "        test_loader=test_loader,\n",
    "        config=config,\n",
    "        output_dir=inference_output_dir,\n",
    "        path_to_student_weight=None,\n",
    "        trainings_id=training_id,\n",
    "        inferenz=True\n",
    "    )\n",
    "\n",
    "    print(f\"Starte Inferenz für Konfiguration: {yaml_path}...\")\n",
    "    auroc_score, total_inference_time = infer.evaluate_quantized_loaded_model()\n",
    "    infer.create_inference_summary(\n",
    "        summary_data, auroc_score, total_inference_time)\n",
    "    print(\n",
    "        f\"Inferenz abgeschlossen. AUROC: {auroc_score:.4f}, Zeit: {total_inference_time:.4f}s.\")\n",
    "\n",
    "    # infer.generate_heatmaps_from_saved_maps()\n",
    "\n",
    "print(\"\\n--- Alle Inferenzläufe abgeschlossen. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b66448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import time  # Hinzugefügt: Modul für die Zeitmessung\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. ONNX-Modell laden\n",
    "onnx_model_path = r\"onnx_models\\STFPM_bottle_mobilenetv4_conv_large.onnx\"\n",
    "sess = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "image_path = glob.glob(os.path.join(\n",
    "    r'Images\\bottle\\test\\broken_large', '*.png'))\n",
    "for path in image_path:\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "    img_size = 256\n",
    "    image = image.resize((img_size, img_size))\n",
    "\n",
    "    input_data = np.array(image, dtype=np.uint8)\n",
    "    input_data = np.expand_dims(input_data, axis=0)\n",
    "\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    outputs = sess.run(None, {input_name: input_data})\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    inference_time = end_time - start_time\n",
    "\n",
    "    anomaly_map = outputs[0]\n",
    "    anomaly_score = outputs[1]\n",
    "\n",
    "    print(f\"Anomalie-Score für das Bild: {anomaly_score[0]}\")\n",
    "    print(f\"Inferenzzeit: {inference_time:.4f} Sekunden\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # 2. Originalbild anzeigen\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Originalbild\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # 3. Heatmap über das Originalbild legen\n",
    "    axes[1].imshow(image)\n",
    "    heatmap = axes[1].imshow(np.squeeze(anomaly_map), cmap='jet', alpha=0.5)\n",
    "    axes[1].set_title(\"Anomalie-Heatmap\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # 4. Farbbalken für die Heatmap hinzufügen\n",
    "    fig.colorbar(heatmap, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    # 5. Plot anzeigen oder speichern\n",
    "    plt.suptitle(\"Vergleich: Originalbild vs. Anomalie-Heatmap\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
